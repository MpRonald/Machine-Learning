{
  "cells":[
    {
      "cell_type":"markdown",
      "source":[
        "# Clustering de dados"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "### K significa agrupamento\n",
        "O agrupamento K-Means é um dos algoritmos mais comumente usados para agrupar dados não rotulados. No agrupamento K-Means, K se refere ao número de clusters nos quais você deseja que seus dados sejam agrupados. No agrupamento K-Means, o número de clusters deve ser definido antes que o agrupamento K possa ser aplicado aos pontos de dados."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "### Etapas para agrupamento de meios K\n",
        "\n",
        "A seguir estão as etapas que devem ser executadas para realizar o agrupamento de K Means de pontos de dados.\n",
        "\n",
        "1. Atribua aleatoriamente valores de centroide para cada cluster.\n",
        "\n",
        "2. Calcule a distância (Euclidiana ou Manhattan) entre cada ponto de dados e valores de centróide de todos os clusters.\n",
        "\n",
        "3. Atribua o ponto de dados ao cluster do centróide com a distância em curto.\n",
        "\n",
        "4. Calcule e atualize os valores do centroide com base nos valores médios das coordenadas de todos os pontos de dados do cluster correspondente.\n",
        "\n",
        "5. Repita as etapas 2–4 até que os novos valores de centróide para todos os clusters sejam diferentes dos valores de centróide anteriores.\n",
        "\n",
        "### Por que usar K Means Clustering?\n",
        "\n",
        "##### O agrupamento de K Means é particularmente útil quando:\n",
        "\n",
        "1. O agrupamento K Means é um algoritmo simples de implementar\n",
        "\n",
        "2. Pode ser aplicado a grandes conjuntos de dados\n",
        "\n",
        "3. Ajusta-se bem a pontos de dados invisíveis\n",
        "\n",
        "4. Generaliza bem para grupos de vários tamanhos e formas."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Clustering Dummy Data com Sklearn"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "from sklearn.datasets._samples_generator import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count":3,
      "outputs":[
        {
          "ename":"ModuleNotFoundError",
          "evalue":"ModuleNotFoundError: No module named 'sklearn.datasets.samples_generator'",
          "traceback":[
            "\u001b[0;31m---------------------------------------------------------------------------",
            "Traceback (most recent call last)",
            "    at line 3 in <module>",
            "ModuleNotFoundError: No module named 'sklearn.datasets.samples_generator'"
          ],
          "output_type":"error"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# gerando dados fictícios de 500 registros com 4 clusters\n",
        "features, labels = make_blobs(n_samples=500, centers=4, cluster_std=20)"
      ],
      "execution_count":4,
      "outputs":[
        {
          "ename":"NameError",
          "evalue":"NameError: name 'make_blobs' is not defined",
          "traceback":[
            "\u001b[0;31m---------------------------------------------------------------------------",
            "Traceback (most recent call last)",
            "    at line 2 in <module>",
            "NameError: name 'make_blobs' is not defined"
          ],
          "output_type":"error"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "plt.scatter(features[:,0], features[:,1])\n",
        "plt.show()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# realizando agrupamento de kmeans usando a classe KMeans\n",
        "\n",
        "km_model = KMeans(n_clusters=4)\n",
        "km_model.fit(features)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# imprimindo os crentroides\n",
        "\n",
        "print(km_model.cluster_centers_)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# imprimindo valores dos labels previstos\n",
        "\n",
        "print(km_model.labels_)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "plt.scatter(features[:,0], features[:,1],\n",
        "           c=km_model.labels_, cmap='rainbow')\n",
        "plt.show()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "plt.scatter(features[:,0], features[:,1],\n",
        "           c=km_model.labels_, cmap='rainbow')\n",
        "\n",
        "plt.scatter(km_model.cluster_centers_[:,0],\n",
        "           km_model.cluster_centers_[:,0],\n",
        "           s=100, c='black')\n",
        "plt.show()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "#### O script a seguir imprime os quatro clusters reais no conjunto de dados."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "plt.scatter(features[:,0], features[:,1],\n",
        "           c=labels, cmap='rainbow')\n",
        "plt.show()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "plt.scatter(features[:,0], features[:,1],\n",
        "           c=labels, cmap='rainbow')\n",
        "plt.show()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Conjunto de dados Iris de clustering"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "import seaborn as sns"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "iris_df = sns.load_dataset(\"iris\")\n",
        "iris_df.head()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# divisao de dados em labels e features\n",
        "\n",
        "features = iris_df.drop([\"species\"], axis = 1)\n",
        "labels = iris_df.filter([\"species\"], axis = 1)\n",
        "features.head()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "#### Vamos primeiro escolher 4 como um número aleatório para o número de clusters. O script a seguir executa o agrupamento de K Means no conjunto de dados Iris."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# modelo KMeans train\n",
        "\n",
        "features = features.values\n",
        "km_model = KMeans(n_clusters=4)\n",
        "km_model.fit(features)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "print(km_model.labels_)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "plt.scatter(features[:,0], features[:,1],\n",
        "            c=km_model.labels_, cmap='rainbow')\n",
        "plt.scatter(km_model.cluster_centers_[:,0],\n",
        "           km_model.cluster_centers_[:,1],\n",
        "           s=100, c='black')\n",
        "plt.show()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "Até agora, neste capítulo, inicializamos aleatoriamente o valor de K ou o número de clusters. No entanto, existe uma maneira de encontrar o número ideal de clusters. O método é conhecido como método do cotovelo. No método do cotovelo, o valor da inércia obtido pelo treinamento de clusters K Means com diferentes números de K é plotado.\n",
        "\n",
        "A inércia representa a distância total entre os pontos de dados dentro de um cluster. Uma inércia menor significa que os clusters previstos são robustos e próximos dos clusters reais.\n",
        "\n",
        "Para calcular o valor de inércia, você pode usar o inertia_attribute do objeto da classe KMeans. O script a seguir cria valores inerciais para K = 1 a 10 e plota na forma de um gráfico de linha, conforme mostrado abaixo:"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "loss = []\n",
        "for i in range(1,11):\n",
        "    km = KMeans(n_clusters=i).fit(features)\n",
        "    loss.append(km.inertia_)\n",
        "    \n",
        "plt.plot(range(1,11), loss)\n",
        "plt.title ('Encontrar agrupamentos ideais através do método do cotovelo')\n",
        "plt.xlabel ('Número de clusters')\n",
        "plt.ylabel ('Loss')\n",
        "plt.show()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "Agora vamos agrupar os dados Iris usando 3 clusters e ver se podemos chegar perto dos clusters reais."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# KMeans train com 3 clusters\n",
        "\n",
        "km_model = KMeans(n_clusters=3)\n",
        "km_model.fit(features)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# pontos de dados com rótulos predeterminados\n",
        "\n",
        "plt.scatter(features[:,0], features[:,1],\n",
        "           c=km_model.labels_,\n",
        "           cmap='rainbow')\n",
        "\n",
        "plt.scatter(km_model.cluster_centers_[:,0],\n",
        "           km_model.cluster_centers_[:,1],\n",
        "           s=100, c='black')\n",
        "plt.show()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "#### Vamos agora representar graficamente os clusters reais e ver o quão próximos os clusters reais estão dos clusters previstos."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Agrupamento hierárquico"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "O agrupamento hierárquico pode ser amplamente dividido em dois tipos: agrupamento aglomerativo e agrupamento divisivo. O clustering aglomerativo segue uma abordagem de baixo para cima, em que pontos de dados individuais são agrupados para formar vários pequenos clusters que levam a um grande cluster, que pode então ser dividido em pequenos clusters usando dendrogramas. Por outro lado, no caso de agrupamento divisivo, você tem um grande cluster, que pode ser dividido em N números de pequenos clusters."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "#### Etapas para agrupamento aglomerativo hierárquico\n",
        "\n",
        "As etapas necessárias para realizar o agrupamento aglomerativo são as seguintes:\n",
        "\n",
        "1. Considere cada ponto de dados no conjunto de dados como um cluster. Portanto, o número de clusters no início é igual ao número de pontos de dados.\n",
        "\n",
        "2. Junte os dois pontos de dados mais próximos para formar um cluster.\n",
        "\n",
        "3. Forme mais clusters unindo-se aos clusters mais próximos. Repita este processo até que um grande cluster seja formado.\n",
        "\n",
        "4. Use dendrogramas para dividir um grande cluster em vários pequenos clusters.\n",
        "\n",
        "#### Por que usar clustering hierárquico?\n",
        "\n",
        "##### O clustering hierárquico tem as seguintes vantagens:\n",
        "\n",
        "1. Ao contrário do agrupamento K Means, para agrupamento hierárquico, você não precisa especificar o número de agrupamento de centroides.\n",
        "\n",
        "2. Com dendrogramas, é mais fácil interpretar como os dados foram agrupados.\n",
        "\n",
        "#### Desvantagens do algoritmo de clustering hierárquico\n",
        "\n",
        "##### A seguir estão algumas das desvantagens do algoritmo de agrupamento hierárquico:\n",
        "\n",
        "1. Não se adapta bem a dados não vistos.\n",
        "\n",
        "2. Tem maior complexidade de tempo em comparação com o agrupamento K Means.\n",
        "\n",
        "3. Difícil determinar o número de clusters no caso de um grande conjunto de dados."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Clustering Dummy Data"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# No primeiro exemplo, realizaremos o agrupamento \n",
        "# aglomerativo de apenas 10 pontos de dados bidimensionais.\n",
        "\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "O script a seguir cria pontos de dados aleatoriamente e, em seguida, rotula os pontos de dados de 1 a 10. Os pontos de dados são plotados como um gráfico de dispersão."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "features, labels = make_blobs(n_samples=10, centers=2, cluster_std=2.00)\n",
        "plt.scatter(features[:,0], features[:,1], color='r')\n",
        "\n",
        "# adição de numeros aos pontos de dados \n",
        "annots = range(1,11)\n",
        "for label, x, y in zip(annots, features[:,0], features[:,1]):\n",
        "    plt.annotate(label, xy=(x,y), xytext=(-3,3),\n",
        "                textcoords='offset points',\n",
        "                ha='right', va='bottom')\n",
        "plt.show()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "##### podemos ver que os pontos 7,10,9,8,2,5 pertencem a um cluster e os 6,3,4,1 a outro cluster"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "Vamos agora plotar dendrogramas para os 10 pontos de dados acima. Para plotar dendrogramas, você pode usar o dendrograma e as classes de ligação do módulo scipy.cluster.hierarchy. Os recursos são passados para a classe de ligação. E o objeto da classe de ligação é passado para a classe de dendrograma para plotar o dendrograma para os recursos, conforme mostrado no seguinte script:"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "from scipy.cluster.hierarchy import dendrogram, linkage "
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "dendos = linkage(features, 'single')\n",
        "annots = range(1,11)\n",
        "dendrogram(dendos, orientation='top', labels=annots,\n",
        "           distance_sort='descending', show_leaf_counts=True)\n",
        "plt.show()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "Na figura acima vemos que os pontos 9 e 8 são os mais proximos um do outro"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "Em cenários do mundo real, pode haver milhares de pontos de dados e, portanto, o método de dendrograma não pode ser usado para agrupar manualmente os dados. É aqui que podemos usar a classe AgglomerativeClustering do módulo sklearn.cluster. O número de clusters e os tipos de distância são passados ​​como parâmetros para a classe AgglomerativeClustering."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "hc_model = AgglomerativeClustering(n_clusters=2, affinity='euclidean',\n",
        "                                   linkage='ward')\n",
        "hc_model.fit_predict(features)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "plt.scatter(features[:,0], features[:,1], c=hc_model.labels_,\n",
        "           cmap='rainbow')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "No exemplo anterior, tínhamos 10 pontos de dados com 2 clusters. Vamos agora ver um exemplo com 500 pontos de dados. O script a seguir cria 500 pontos de dados com 4 centros de cluster."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# gerando dados fictícios de 500 registros com 4 clusters\n",
        "\n",
        "features, labels = make_blobs(n_samples=500, centers=4, cluster_std=2.00)\n",
        "plt.scatter(features[:,0], features[:,1])\n",
        "plt.show()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# executando agrupamento de kmeans usando \n",
        "# a classe AgglomerativeClustering\n",
        "\n",
        "hc_model = AgglomerativeClustering(n_clusters=4, affinity='euclidean',\n",
        "                                  linkage='ward')\n",
        "hc_model.fit_predict(features)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "plt.scatter(features[:,0], features[:,1], c= hc_model.labels_, cmap='rainbow')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "plt.scatter(features[:,0], features[:,1], c= labels, cmap='rainbow')"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "#  Clustering o Iris Dataset"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "import seaborn as sns\n",
        "iris_df = sns.load_dataset('iris')\n",
        "iris_df.head()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "features = iris_df.drop(['species'], axis = 1)\n",
        "labels = iris_df.filter(['species'], axis = 1)\n",
        "features.head()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "from sklearn.cluster import AgglomerativeClustering\n",
        "features = features.values\n",
        "hc_model = AgglomerativeClustering(n_clusters=3, affinity='euclidean',\n",
        "                                   linkage='ward')\n",
        "hc_model.fit_predict(features)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "plt.scatter(features[:,0], features[:,1], c= hc_model.labels_,\n",
        "            cmap='rainbow' )"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "Você também pode criar dendrogramas usando o conjunto de recursos usando o módulo shc da biblioteca scipy.cluster.hierarchy. Você deve passar o conjunto de recursos para a classe de ligação do módulo shc e, em seguida, o objeto da classe de ligação é passado para a classe de dendrograma para plotar os dendrogramas, conforme mostrado no script a seguir."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "import scipy.cluster.hierarchy as shc\n",
        "plt.figure (figsize = (10, 7))\n",
        "plt.title ( 'Dendogramas de íris' )\n",
        "dend = shc.dendrogram (shc.linkage (features, method = 'ward' ))"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    }
  ],
  "metadata":{
    
  },
  "nbformat":4,
  "nbformat_minor":0
}