{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4ak7VJu7QjB1g+nHwUc7M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MpRonald/Machine-Learning/blob/main/Binary_classifier_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "833IgY21CVKy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_in_breast = pd.read_csv('https://github.com/MpRonald/datasets/raw/main/entradas_breast.csv')\n",
        "data_in_breast"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "plvpv-QtD8e6",
        "outputId": "6f8c4d44-8ed3-44a1-9960-c9bfb57a8529"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      radius_mean   texture_mean   perimeter_mean   area_mean  \\\n",
              "0           17.99          10.38           122.80      1001.0   \n",
              "1           20.57          17.77           132.90      1326.0   \n",
              "2           19.69          21.25           130.00      1203.0   \n",
              "3           11.42          20.38            77.58       386.1   \n",
              "4           20.29          14.34           135.10      1297.0   \n",
              "..            ...            ...              ...         ...   \n",
              "564         21.56          22.39           142.00      1479.0   \n",
              "565         20.13          28.25           131.20      1261.0   \n",
              "566         16.60          28.08           108.30       858.1   \n",
              "567         20.60          29.33           140.10      1265.0   \n",
              "568          7.76          24.54            47.92       181.0   \n",
              "\n",
              "      smoothness_mean   compactness_mean   concavity_mean  \\\n",
              "0             0.11840            0.27760          0.30010   \n",
              "1             0.08474            0.07864          0.08690   \n",
              "2             0.10960            0.15990          0.19740   \n",
              "3             0.14250            0.28390          0.24140   \n",
              "4             0.10030            0.13280        198.00000   \n",
              "..                ...                ...              ...   \n",
              "564         111.00000            0.11590          0.24390   \n",
              "565           0.09780            0.10340        144.00000   \n",
              "566           0.08455            0.10230          0.09251   \n",
              "567           0.11780          277.00000          0.35140   \n",
              "568           0.05263            0.04362          0.00000   \n",
              "\n",
              "     concave_points_mean   symmetry_mean   fractal_dimension_mean  ...  \\\n",
              "0                0.14710          0.2419                  0.07871  ...   \n",
              "1                0.07017          0.1812                  0.05667  ...   \n",
              "2                0.12790          0.2069                  0.05999  ...   \n",
              "3                0.10520          0.2597                  0.09744  ...   \n",
              "4                0.10430          0.1809                  0.05883  ...   \n",
              "..                   ...             ...                      ...  ...   \n",
              "564              0.13890          0.1726                  0.05623  ...   \n",
              "565              0.09791          0.1752                  0.05533  ...   \n",
              "566              0.05302        159.0000                  0.05648  ...   \n",
              "567            152.00000          0.2397                  0.07016  ...   \n",
              "568              0.00000          0.1587                  0.05884  ...   \n",
              "\n",
              "      radius_worst   texture_worst   perimeter_worst   area_worst  \\\n",
              "0            25.38           17.33            184.60       2019.0   \n",
              "1            24.99           23.41            158.80       1956.0   \n",
              "2            23.57           25.53            152.50       1709.0   \n",
              "3            14.91           26.50             98.87        567.7   \n",
              "4            22.54           16.67            152.20       1575.0   \n",
              "..             ...             ...               ...          ...   \n",
              "564          25.45           26.40            166.10       2027.0   \n",
              "565          23.69           38.25            155.00       1731.0   \n",
              "566          18.98           34.12            126.70       1124.0   \n",
              "567          25.74           39.42            184.60       1821.0   \n",
              "568        9456.00           30.37             59.16        268.6   \n",
              "\n",
              "      smoothness_worst   compactness_worst   concavity_worst  \\\n",
              "0              0.16220             0.66560            0.7119   \n",
              "1              0.12380             0.18660            0.2416   \n",
              "2              0.14440             0.42450            0.4504   \n",
              "3              0.20980             0.86630            0.6869   \n",
              "4              0.13740           205.00000            0.4000   \n",
              "..                 ...                 ...               ...   \n",
              "564          141.00000             0.21130            0.4107   \n",
              "565            0.11660             0.19220            0.3215   \n",
              "566            0.11390             0.30940            0.3403   \n",
              "567          165.00000             0.86810            0.9387   \n",
              "568            0.08996             0.06444            0.0000   \n",
              "\n",
              "      concave_points_worst   symmetry_worst   fractal_dimension_worst  \n",
              "0                   0.2654           0.4601                   0.11890  \n",
              "1                 186.0000         275.0000                   0.08902  \n",
              "2                 243.0000           0.3613                   0.08758  \n",
              "3                   0.2575           0.6638                 173.00000  \n",
              "4                   0.1625           0.2364                   0.07678  \n",
              "..                     ...              ...                       ...  \n",
              "564                 0.2216         206.0000                   0.07115  \n",
              "565                 0.1628           0.2572                   0.06637  \n",
              "566                 0.1418           0.2218                   0.07820  \n",
              "567               265.0000           0.4087                 124.00000  \n",
              "568                 0.0000           0.2871                   0.07039  \n",
              "\n",
              "[569 rows x 30 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6e3d60c5-e0ac-4148-9565-ed734a91b833\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave_points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave_points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>...</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>...</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>186.0000</td>\n",
              "      <td>275.0000</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>...</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>243.0000</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>...</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>173.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>198.00000</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>...</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>205.00000</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>111.00000</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>...</td>\n",
              "      <td>25.45</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>141.00000</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>206.0000</td>\n",
              "      <td>0.07115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>144.00000</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>...</td>\n",
              "      <td>23.69</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>159.0000</td>\n",
              "      <td>0.05648</td>\n",
              "      <td>...</td>\n",
              "      <td>18.98</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>277.00000</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>152.00000</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>...</td>\n",
              "      <td>25.74</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>165.00000</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>265.0000</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>124.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>...</td>\n",
              "      <td>9456.00</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows × 30 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e3d60c5-e0ac-4148-9565-ed734a91b833')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6e3d60c5-e0ac-4148-9565-ed734a91b833 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6e3d60c5-e0ac-4148-9565-ed734a91b833');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_out_breast = pd.read_csv('https://github.com/MpRonald/datasets/raw/main/saidas_breast.csv')\n",
        "data_out_breast"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "T-DWIoh5EEOw",
        "outputId": "180602f4-0ad3-4464-faf8-254eee9ad201"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     0\n",
              "0    0\n",
              "1    0\n",
              "2    0\n",
              "3    0\n",
              "4    0\n",
              "..  ..\n",
              "564  0\n",
              "565  0\n",
              "566  0\n",
              "567  0\n",
              "568  1\n",
              "\n",
              "[569 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4f7e911a-3ff4-41e2-9cc8-cf1cec048c16\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f7e911a-3ff4-41e2-9cc8-cf1cec048c16')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4f7e911a-3ff4-41e2-9cc8-cf1cec048c16 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4f7e911a-3ff4-41e2-9cc8-cf1cec048c16');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data_in_breast, data_out_breast, test_size=.2, random_state=42)"
      ],
      "metadata": {
        "id": "Kf40qa16ETNZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTNCCOWwE5GC",
        "outputId": "1294e233-ce11-4c71-9188-0055eb0dd640"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((455, 30), (114, 30), (455, 1), (114, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = Sequential()\n",
        "clf.add(Dense(units=16, activation='relu',\n",
        "                kernel_initializer='random_uniform',\n",
        "                    input_dim=30))\n",
        "clf.add(Dense(units=16, activation='relu', kernel_initializer='random_uniform'))\n",
        "clf.add(Dense(units=1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "g99h9uQdFDYQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optim = keras.optimizers.Adamax(lr = 0.001, decay=0.0001)\n",
        "clf.compile(optimizer=optim, loss='binary_crossentropy', metrics=['binary_accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz13UdB7HpBx",
        "outputId": "3f2ed596-591c-41a5-e40c-72144e3be565"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adamax.py:95: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adamax, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.fit(X_train, y_train, batch_size=15, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSAnimLNImnP",
        "outputId": "37eb68d5-0fe3-4c80-c798-62eab7318f20"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "31/31 [==============================] - 2s 3ms/step - loss: 1.0602 - binary_accuracy: 0.5692\n",
            "Epoch 2/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.5813 - binary_accuracy: 0.6308\n",
            "Epoch 3/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.5262 - binary_accuracy: 0.6637\n",
            "Epoch 4/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.5039 - binary_accuracy: 0.6923\n",
            "Epoch 5/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.4827 - binary_accuracy: 0.7253\n",
            "Epoch 6/100\n",
            "31/31 [==============================] - 0s 4ms/step - loss: 0.4641 - binary_accuracy: 0.7275\n",
            "Epoch 7/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.4532 - binary_accuracy: 0.7538\n",
            "Epoch 8/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.4364 - binary_accuracy: 0.7934\n",
            "Epoch 9/100\n",
            "31/31 [==============================] - 0s 4ms/step - loss: 0.4261 - binary_accuracy: 0.8154\n",
            "Epoch 10/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.4124 - binary_accuracy: 0.8154\n",
            "Epoch 11/100\n",
            "31/31 [==============================] - 0s 8ms/step - loss: 0.4178 - binary_accuracy: 0.8088\n",
            "Epoch 12/100\n",
            "31/31 [==============================] - 0s 4ms/step - loss: 0.3978 - binary_accuracy: 0.8198\n",
            "Epoch 13/100\n",
            "31/31 [==============================] - 0s 4ms/step - loss: 0.3899 - binary_accuracy: 0.8286\n",
            "Epoch 14/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.3891 - binary_accuracy: 0.8330\n",
            "Epoch 15/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.3709 - binary_accuracy: 0.8549\n",
            "Epoch 16/100\n",
            "31/31 [==============================] - 0s 4ms/step - loss: 0.3772 - binary_accuracy: 0.8527\n",
            "Epoch 17/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.3595 - binary_accuracy: 0.8418\n",
            "Epoch 18/100\n",
            "31/31 [==============================] - 0s 8ms/step - loss: 0.3388 - binary_accuracy: 0.8725\n",
            "Epoch 19/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.3198 - binary_accuracy: 0.8835\n",
            "Epoch 20/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.3163 - binary_accuracy: 0.8637\n",
            "Epoch 21/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.3068 - binary_accuracy: 0.8835\n",
            "Epoch 22/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.3115 - binary_accuracy: 0.8725\n",
            "Epoch 23/100\n",
            "31/31 [==============================] - 0s 4ms/step - loss: 0.2925 - binary_accuracy: 0.8769\n",
            "Epoch 24/100\n",
            "31/31 [==============================] - 0s 4ms/step - loss: 0.2795 - binary_accuracy: 0.8945\n",
            "Epoch 25/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.2717 - binary_accuracy: 0.8945\n",
            "Epoch 26/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.2841 - binary_accuracy: 0.8769\n",
            "Epoch 27/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.2783 - binary_accuracy: 0.8747\n",
            "Epoch 28/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.2488 - binary_accuracy: 0.9077\n",
            "Epoch 29/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.2522 - binary_accuracy: 0.8901\n",
            "Epoch 30/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.2603 - binary_accuracy: 0.9099\n",
            "Epoch 31/100\n",
            "31/31 [==============================] - 0s 4ms/step - loss: 0.2387 - binary_accuracy: 0.9033\n",
            "Epoch 32/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.2368 - binary_accuracy: 0.9143\n",
            "Epoch 33/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.2376 - binary_accuracy: 0.9099\n",
            "Epoch 34/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.2248 - binary_accuracy: 0.9143\n",
            "Epoch 35/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.2331 - binary_accuracy: 0.9121\n",
            "Epoch 36/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.2365 - binary_accuracy: 0.9077\n",
            "Epoch 37/100\n",
            "31/31 [==============================] - 0s 4ms/step - loss: 0.2234 - binary_accuracy: 0.9121\n",
            "Epoch 38/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.2168 - binary_accuracy: 0.9121\n",
            "Epoch 39/100\n",
            "31/31 [==============================] - 0s 4ms/step - loss: 0.2159 - binary_accuracy: 0.9143\n",
            "Epoch 40/100\n",
            "31/31 [==============================] - 0s 8ms/step - loss: 0.2169 - binary_accuracy: 0.9143\n",
            "Epoch 41/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.2014 - binary_accuracy: 0.9209\n",
            "Epoch 42/100\n",
            "31/31 [==============================] - 0s 8ms/step - loss: 0.2162 - binary_accuracy: 0.9209\n",
            "Epoch 43/100\n",
            "31/31 [==============================] - 0s 4ms/step - loss: 0.2052 - binary_accuracy: 0.9231\n",
            "Epoch 44/100\n",
            "31/31 [==============================] - 0s 4ms/step - loss: 0.2042 - binary_accuracy: 0.9143\n",
            "Epoch 45/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.2008 - binary_accuracy: 0.9231\n",
            "Epoch 46/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.2243 - binary_accuracy: 0.9187\n",
            "Epoch 47/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.2088 - binary_accuracy: 0.9209\n",
            "Epoch 48/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.2068 - binary_accuracy: 0.9187\n",
            "Epoch 49/100\n",
            "31/31 [==============================] - 0s 4ms/step - loss: 0.2051 - binary_accuracy: 0.9165\n",
            "Epoch 50/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.1979 - binary_accuracy: 0.9275\n",
            "Epoch 51/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.1986 - binary_accuracy: 0.9165\n",
            "Epoch 52/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.1935 - binary_accuracy: 0.9209\n",
            "Epoch 53/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.1905 - binary_accuracy: 0.9297\n",
            "Epoch 54/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.1885 - binary_accuracy: 0.9297\n",
            "Epoch 55/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.1896 - binary_accuracy: 0.9231\n",
            "Epoch 56/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.1873 - binary_accuracy: 0.9253\n",
            "Epoch 57/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.1875 - binary_accuracy: 0.9187\n",
            "Epoch 58/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.1943 - binary_accuracy: 0.9253\n",
            "Epoch 59/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.1879 - binary_accuracy: 0.9363\n",
            "Epoch 60/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.1947 - binary_accuracy: 0.9187\n",
            "Epoch 61/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.1767 - binary_accuracy: 0.9297\n",
            "Epoch 62/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.2063 - binary_accuracy: 0.9121\n",
            "Epoch 63/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.1806 - binary_accuracy: 0.9253\n",
            "Epoch 64/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.1776 - binary_accuracy: 0.9297\n",
            "Epoch 65/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.1926 - binary_accuracy: 0.9099\n",
            "Epoch 66/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.1972 - binary_accuracy: 0.9165\n",
            "Epoch 67/100\n",
            "31/31 [==============================] - 0s 4ms/step - loss: 0.1923 - binary_accuracy: 0.9209\n",
            "Epoch 68/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.1791 - binary_accuracy: 0.9231\n",
            "Epoch 69/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.1794 - binary_accuracy: 0.9275\n",
            "Epoch 70/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.1740 - binary_accuracy: 0.9341\n",
            "Epoch 71/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.1743 - binary_accuracy: 0.9363\n",
            "Epoch 72/100\n",
            "31/31 [==============================] - 0s 4ms/step - loss: 0.1716 - binary_accuracy: 0.9385\n",
            "Epoch 73/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.1714 - binary_accuracy: 0.9429\n",
            "Epoch 74/100\n",
            "31/31 [==============================] - 0s 6ms/step - loss: 0.1710 - binary_accuracy: 0.9275\n",
            "Epoch 75/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.1675 - binary_accuracy: 0.9209\n",
            "Epoch 76/100\n",
            "31/31 [==============================] - 0s 4ms/step - loss: 0.1887 - binary_accuracy: 0.9297\n",
            "Epoch 77/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.1756 - binary_accuracy: 0.9253\n",
            "Epoch 78/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.1829 - binary_accuracy: 0.9297\n",
            "Epoch 79/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.1726 - binary_accuracy: 0.9319\n",
            "Epoch 80/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.1649 - binary_accuracy: 0.9341\n",
            "Epoch 81/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.1791 - binary_accuracy: 0.9341\n",
            "Epoch 82/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.1668 - binary_accuracy: 0.9319\n",
            "Epoch 83/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.1653 - binary_accuracy: 0.9297\n",
            "Epoch 84/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.1535 - binary_accuracy: 0.9495\n",
            "Epoch 85/100\n",
            "31/31 [==============================] - 0s 4ms/step - loss: 0.1759 - binary_accuracy: 0.9209\n",
            "Epoch 86/100\n",
            "31/31 [==============================] - 0s 4ms/step - loss: 0.1707 - binary_accuracy: 0.9275\n",
            "Epoch 87/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.1619 - binary_accuracy: 0.9363\n",
            "Epoch 88/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.1682 - binary_accuracy: 0.9275\n",
            "Epoch 89/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.1596 - binary_accuracy: 0.9429\n",
            "Epoch 90/100\n",
            "31/31 [==============================] - 0s 5ms/step - loss: 0.1555 - binary_accuracy: 0.9385\n",
            "Epoch 91/100\n",
            "31/31 [==============================] - 0s 4ms/step - loss: 0.1709 - binary_accuracy: 0.9341\n",
            "Epoch 92/100\n",
            "31/31 [==============================] - 0s 8ms/step - loss: 0.1765 - binary_accuracy: 0.9253\n",
            "Epoch 93/100\n",
            "31/31 [==============================] - 0s 7ms/step - loss: 0.1571 - binary_accuracy: 0.9341\n",
            "Epoch 94/100\n",
            "31/31 [==============================] - 0s 4ms/step - loss: 0.1581 - binary_accuracy: 0.9341\n",
            "Epoch 95/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.1701 - binary_accuracy: 0.9253\n",
            "Epoch 96/100\n",
            "31/31 [==============================] - 0s 2ms/step - loss: 0.1519 - binary_accuracy: 0.9473\n",
            "Epoch 97/100\n",
            "31/31 [==============================] - 0s 2ms/step - loss: 0.1603 - binary_accuracy: 0.9363\n",
            "Epoch 98/100\n",
            "31/31 [==============================] - 0s 2ms/step - loss: 0.1516 - binary_accuracy: 0.9407\n",
            "Epoch 99/100\n",
            "31/31 [==============================] - 0s 2ms/step - loss: 0.1530 - binary_accuracy: 0.9385\n",
            "Epoch 100/100\n",
            "31/31 [==============================] - 0s 3ms/step - loss: 0.1527 - binary_accuracy: 0.9341\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7feeed12a640>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "y_pred = y_pred > 0.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVSH6rcUJBIf",
        "outputId": "e38c87d3-d455-48b2-d4d2-e4af738bfa37"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_test = accuracy_score(y_test, y_pred)\n",
        "predict_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz1JKvBNLFJI",
        "outputId": "0e7a503f-a9d9-4311-96e7-1abedc04665d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9210526315789473"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "conf_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mx8r1AkILn-g",
        "outputId": "bf46438f-b098-48a3-c32e-d891419ecac0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[37,  6],\n",
              "       [ 3, 68]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x1nxAc0NCOv",
        "outputId": "5c61ea0f-1caa-4ccb-b75a-ef026d00a337"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.86      0.89        43\n",
            "           1       0.92      0.96      0.94        71\n",
            "\n",
            "    accuracy                           0.92       114\n",
            "   macro avg       0.92      0.91      0.91       114\n",
            "weighted avg       0.92      0.92      0.92       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_neural_network():\n",
        "    clf = Sequential()\n",
        "    clf.add(Dense(units=16, activation='relu',\n",
        "                    kernel_initializer='random_uniform',\n",
        "                        input_dim=30))\n",
        "    clf.add(Dropout(.25))\n",
        "    clf.add(Dense(units=16, activation='relu', kernel_initializer='random_uniform'))\n",
        "    clf.add(Dropout(.15))\n",
        "    clf.add(Dense(units=1, activation='sigmoid'))\n",
        "    optim = keras.optimizers.Adamax(lr = 0.001, decay=0.0001)\n",
        "    clf.compile(optimizer=optim, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
        "\n",
        "    return clf"
      ],
      "metadata": {
        "id": "KwqjoIdvNq8P"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_nn = KerasClassifier(build_fn=create_neural_network, epochs=100, batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCvzyV2kS-hs",
        "outputId": "65b8f1ae-f74c-4a4f-b439-764824557fd6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-05d7f4d9f368>:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  clf_nn = KerasClassifier(build_fn=create_neural_network, epochs=100, batch_size=10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = cross_val_score(estimator=clf_nn, X=data_in_breast, y=data_out_breast,\n",
        "                          cv=5, scoring='accuracy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A46OqoBZTNlI",
        "outputId": "2777cd7f-88d3-4c83-d885-e639c6fdee55"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adamax.py:95: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adamax, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46/46 [==============================] - 1s 2ms/step - loss: 1.7841 - binary_accuracy: 0.5868\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 1.1472 - binary_accuracy: 0.6286\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.7780 - binary_accuracy: 0.6396\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.7417 - binary_accuracy: 0.6176\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.6706 - binary_accuracy: 0.6571\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.6358 - binary_accuracy: 0.6747\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5828 - binary_accuracy: 0.7077\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.6052 - binary_accuracy: 0.6813\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5753 - binary_accuracy: 0.7275\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.6078 - binary_accuracy: 0.6923\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5527 - binary_accuracy: 0.7055\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5458 - binary_accuracy: 0.6879\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5531 - binary_accuracy: 0.7275\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5533 - binary_accuracy: 0.7165\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4992 - binary_accuracy: 0.7604\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5389 - binary_accuracy: 0.7429\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5134 - binary_accuracy: 0.7495\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4932 - binary_accuracy: 0.7824\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4875 - binary_accuracy: 0.7758\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4515 - binary_accuracy: 0.7802\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4414 - binary_accuracy: 0.8044\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4079 - binary_accuracy: 0.8308\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4131 - binary_accuracy: 0.8066\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3945 - binary_accuracy: 0.8220\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3962 - binary_accuracy: 0.8440\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3716 - binary_accuracy: 0.8637\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3673 - binary_accuracy: 0.8593\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3829 - binary_accuracy: 0.8549\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3675 - binary_accuracy: 0.8571\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3500 - binary_accuracy: 0.8769\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3466 - binary_accuracy: 0.8593\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3477 - binary_accuracy: 0.8615\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3102 - binary_accuracy: 0.9033\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3130 - binary_accuracy: 0.8923\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3286 - binary_accuracy: 0.8659\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3178 - binary_accuracy: 0.8791\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3333 - binary_accuracy: 0.8879\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3157 - binary_accuracy: 0.8681\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3005 - binary_accuracy: 0.8879\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2884 - binary_accuracy: 0.8967\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3082 - binary_accuracy: 0.8989\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2919 - binary_accuracy: 0.9011\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2903 - binary_accuracy: 0.9055\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3101 - binary_accuracy: 0.8967\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2666 - binary_accuracy: 0.9121\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2794 - binary_accuracy: 0.9055\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2776 - binary_accuracy: 0.9055\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2795 - binary_accuracy: 0.9187\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2532 - binary_accuracy: 0.9033\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2508 - binary_accuracy: 0.9099\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2929 - binary_accuracy: 0.8813\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2522 - binary_accuracy: 0.9231\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2430 - binary_accuracy: 0.9231\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2507 - binary_accuracy: 0.9055\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2487 - binary_accuracy: 0.9077\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2695 - binary_accuracy: 0.8879\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2606 - binary_accuracy: 0.9099\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2294 - binary_accuracy: 0.9231\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2383 - binary_accuracy: 0.9099\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2529 - binary_accuracy: 0.9099\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2212 - binary_accuracy: 0.9231\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2287 - binary_accuracy: 0.9121\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2463 - binary_accuracy: 0.9033\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2213 - binary_accuracy: 0.9187\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2212 - binary_accuracy: 0.9077\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2109 - binary_accuracy: 0.9253\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2200 - binary_accuracy: 0.9275\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2351 - binary_accuracy: 0.9209\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2395 - binary_accuracy: 0.9187\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2433 - binary_accuracy: 0.9143\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2353 - binary_accuracy: 0.9121\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2304 - binary_accuracy: 0.9187\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2160 - binary_accuracy: 0.9275\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2243 - binary_accuracy: 0.9275\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2388 - binary_accuracy: 0.9143\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2115 - binary_accuracy: 0.9231\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.1946 - binary_accuracy: 0.9319\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2139 - binary_accuracy: 0.9275\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2302 - binary_accuracy: 0.9121\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.1994 - binary_accuracy: 0.9341\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2035 - binary_accuracy: 0.9253\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2040 - binary_accuracy: 0.9319\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.1913 - binary_accuracy: 0.9363\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.1843 - binary_accuracy: 0.9473\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.1929 - binary_accuracy: 0.9385\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2023 - binary_accuracy: 0.9275\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.1890 - binary_accuracy: 0.9319\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2123 - binary_accuracy: 0.9297\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.1867 - binary_accuracy: 0.9319\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.1827 - binary_accuracy: 0.9297\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.1966 - binary_accuracy: 0.9319\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.1860 - binary_accuracy: 0.9275\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.1993 - binary_accuracy: 0.9319\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2021 - binary_accuracy: 0.9165\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.1911 - binary_accuracy: 0.9363\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.1870 - binary_accuracy: 0.9297\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2010 - binary_accuracy: 0.9341\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.1819 - binary_accuracy: 0.9407\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2065 - binary_accuracy: 0.9319\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.1809 - binary_accuracy: 0.9319\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adamax.py:95: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adamax, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46/46 [==============================] - 1s 2ms/step - loss: 1.8547 - binary_accuracy: 0.5099\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.9954 - binary_accuracy: 0.5912\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.7411 - binary_accuracy: 0.6110\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.6810 - binary_accuracy: 0.6220\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.7556 - binary_accuracy: 0.6154\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.6524 - binary_accuracy: 0.6000\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.6583 - binary_accuracy: 0.6462\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.6077 - binary_accuracy: 0.6769\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5781 - binary_accuracy: 0.6440\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5828 - binary_accuracy: 0.6835\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5373 - binary_accuracy: 0.6813\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4958 - binary_accuracy: 0.7363\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4930 - binary_accuracy: 0.7297\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5649 - binary_accuracy: 0.7407\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4749 - binary_accuracy: 0.7516\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4637 - binary_accuracy: 0.7582\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4705 - binary_accuracy: 0.8044\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4642 - binary_accuracy: 0.7912\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4659 - binary_accuracy: 0.7824\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4387 - binary_accuracy: 0.7780\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4344 - binary_accuracy: 0.8044\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4310 - binary_accuracy: 0.8088\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4398 - binary_accuracy: 0.8330\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4087 - binary_accuracy: 0.8330\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3972 - binary_accuracy: 0.8352\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3997 - binary_accuracy: 0.8308\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3903 - binary_accuracy: 0.8462\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3922 - binary_accuracy: 0.8396\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3638 - binary_accuracy: 0.8659\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3888 - binary_accuracy: 0.8308\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3485 - binary_accuracy: 0.8615\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3501 - binary_accuracy: 0.8703\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3557 - binary_accuracy: 0.8418\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3465 - binary_accuracy: 0.8637\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3678 - binary_accuracy: 0.8308\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3365 - binary_accuracy: 0.8615\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3448 - binary_accuracy: 0.8484\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3490 - binary_accuracy: 0.8593\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3141 - binary_accuracy: 0.8703\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3253 - binary_accuracy: 0.8791\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3098 - binary_accuracy: 0.8813\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3101 - binary_accuracy: 0.8791\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3148 - binary_accuracy: 0.8769\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3181 - binary_accuracy: 0.8769\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2887 - binary_accuracy: 0.9011\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3024 - binary_accuracy: 0.8813\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3031 - binary_accuracy: 0.8747\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3148 - binary_accuracy: 0.8747\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3047 - binary_accuracy: 0.8747\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3274 - binary_accuracy: 0.8637\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2842 - binary_accuracy: 0.8945\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2996 - binary_accuracy: 0.8967\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2756 - binary_accuracy: 0.8879\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2784 - binary_accuracy: 0.8879\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2871 - binary_accuracy: 0.8813\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2746 - binary_accuracy: 0.8967\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2604 - binary_accuracy: 0.9099\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2643 - binary_accuracy: 0.9077\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2856 - binary_accuracy: 0.8901\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2573 - binary_accuracy: 0.9077\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2644 - binary_accuracy: 0.8857\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3017 - binary_accuracy: 0.8725\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2527 - binary_accuracy: 0.9077\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3052 - binary_accuracy: 0.8593\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2679 - binary_accuracy: 0.8967\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2678 - binary_accuracy: 0.8901\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2436 - binary_accuracy: 0.9033\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2772 - binary_accuracy: 0.8923\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2479 - binary_accuracy: 0.9011\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2544 - binary_accuracy: 0.8989\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2574 - binary_accuracy: 0.8923\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2362 - binary_accuracy: 0.9165\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2334 - binary_accuracy: 0.8989\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2404 - binary_accuracy: 0.9033\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2390 - binary_accuracy: 0.9099\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2632 - binary_accuracy: 0.8945\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2617 - binary_accuracy: 0.8901\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2423 - binary_accuracy: 0.8857\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2373 - binary_accuracy: 0.9033\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2348 - binary_accuracy: 0.8967\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2621 - binary_accuracy: 0.9165\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2095 - binary_accuracy: 0.9209\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2439 - binary_accuracy: 0.9143\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2245 - binary_accuracy: 0.9099\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2349 - binary_accuracy: 0.9121\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2303 - binary_accuracy: 0.9121\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2341 - binary_accuracy: 0.9099\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2306 - binary_accuracy: 0.9143\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2586 - binary_accuracy: 0.8989\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2268 - binary_accuracy: 0.9099\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2470 - binary_accuracy: 0.8901\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2376 - binary_accuracy: 0.9055\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2343 - binary_accuracy: 0.9165\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2160 - binary_accuracy: 0.9099\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2362 - binary_accuracy: 0.8989\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2400 - binary_accuracy: 0.9143\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2076 - binary_accuracy: 0.9165\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2293 - binary_accuracy: 0.8879\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2158 - binary_accuracy: 0.9231\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2228 - binary_accuracy: 0.9099\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adamax.py:95: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adamax, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46/46 [==============================] - 1s 2ms/step - loss: 1.8402 - binary_accuracy: 0.5538\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0917 - binary_accuracy: 0.5670\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.9368 - binary_accuracy: 0.5912\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.8202 - binary_accuracy: 0.5714\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.6781 - binary_accuracy: 0.6505\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.6874 - binary_accuracy: 0.5780\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.6842 - binary_accuracy: 0.5912\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5663 - binary_accuracy: 0.6374\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.6066 - binary_accuracy: 0.6154\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.6023 - binary_accuracy: 0.6264\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5494 - binary_accuracy: 0.6418\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5959 - binary_accuracy: 0.6176\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5722 - binary_accuracy: 0.6418\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5653 - binary_accuracy: 0.6374\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5206 - binary_accuracy: 0.6374\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5417 - binary_accuracy: 0.6593\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5179 - binary_accuracy: 0.6901\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5582 - binary_accuracy: 0.6396\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5305 - binary_accuracy: 0.6681\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5199 - binary_accuracy: 0.6835\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5324 - binary_accuracy: 0.6527\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4921 - binary_accuracy: 0.6835\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.5000 - binary_accuracy: 0.7385\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5020 - binary_accuracy: 0.7187\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5115 - binary_accuracy: 0.7055\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4496 - binary_accuracy: 0.7495\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4869 - binary_accuracy: 0.7538\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4935 - binary_accuracy: 0.7341\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4664 - binary_accuracy: 0.7736\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4681 - binary_accuracy: 0.7736\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4487 - binary_accuracy: 0.7758\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4388 - binary_accuracy: 0.7846\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4347 - binary_accuracy: 0.8000\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4302 - binary_accuracy: 0.7912\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4223 - binary_accuracy: 0.7934\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4305 - binary_accuracy: 0.8022\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4173 - binary_accuracy: 0.8220\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4575 - binary_accuracy: 0.7736\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4137 - binary_accuracy: 0.8132\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4116 - binary_accuracy: 0.8000\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4102 - binary_accuracy: 0.8000\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4148 - binary_accuracy: 0.7956\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3734 - binary_accuracy: 0.8088\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3787 - binary_accuracy: 0.8286\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3754 - binary_accuracy: 0.8308\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3606 - binary_accuracy: 0.8440\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3671 - binary_accuracy: 0.8352\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3397 - binary_accuracy: 0.8593\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3546 - binary_accuracy: 0.8484\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3848 - binary_accuracy: 0.8396\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3610 - binary_accuracy: 0.8593\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3668 - binary_accuracy: 0.8308\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3373 - binary_accuracy: 0.8418\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3744 - binary_accuracy: 0.8549\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3345 - binary_accuracy: 0.8527\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3443 - binary_accuracy: 0.8462\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3332 - binary_accuracy: 0.8593\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3279 - binary_accuracy: 0.8747\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3609 - binary_accuracy: 0.8440\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3663 - binary_accuracy: 0.8857\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2968 - binary_accuracy: 0.8879\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3234 - binary_accuracy: 0.8681\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3350 - binary_accuracy: 0.8703\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3104 - binary_accuracy: 0.8835\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3031 - binary_accuracy: 0.8857\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3056 - binary_accuracy: 0.8769\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3184 - binary_accuracy: 0.8769\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3010 - binary_accuracy: 0.8681\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3030 - binary_accuracy: 0.8769\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2984 - binary_accuracy: 0.8769\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3079 - binary_accuracy: 0.8923\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2665 - binary_accuracy: 0.8989\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3004 - binary_accuracy: 0.8747\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3013 - binary_accuracy: 0.8659\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2697 - binary_accuracy: 0.8989\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2977 - binary_accuracy: 0.8791\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2929 - binary_accuracy: 0.8835\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2914 - binary_accuracy: 0.8813\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2762 - binary_accuracy: 0.8747\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3000 - binary_accuracy: 0.8769\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2675 - binary_accuracy: 0.8923\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2827 - binary_accuracy: 0.8747\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2728 - binary_accuracy: 0.8813\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2863 - binary_accuracy: 0.8813\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2758 - binary_accuracy: 0.8813\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2592 - binary_accuracy: 0.9099\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2771 - binary_accuracy: 0.8769\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3034 - binary_accuracy: 0.8857\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2630 - binary_accuracy: 0.9143\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2455 - binary_accuracy: 0.9055\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2562 - binary_accuracy: 0.8901\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2296 - binary_accuracy: 0.9209\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2713 - binary_accuracy: 0.8835\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2564 - binary_accuracy: 0.8945\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2598 - binary_accuracy: 0.8945\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2682 - binary_accuracy: 0.8857\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2810 - binary_accuracy: 0.8901\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2494 - binary_accuracy: 0.9055\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2391 - binary_accuracy: 0.9055\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2538 - binary_accuracy: 0.9055\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adamax.py:95: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adamax, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46/46 [==============================] - 1s 2ms/step - loss: 1.2817 - binary_accuracy: 0.5692\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.8190 - binary_accuracy: 0.5868\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.7319 - binary_accuracy: 0.6088\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.6081 - binary_accuracy: 0.6286\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.6106 - binary_accuracy: 0.6747\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.5510 - binary_accuracy: 0.7187\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.6260 - binary_accuracy: 0.6879\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5188 - binary_accuracy: 0.7297\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5619 - binary_accuracy: 0.7011\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5315 - binary_accuracy: 0.6967\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5293 - binary_accuracy: 0.7736\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5171 - binary_accuracy: 0.7275\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5128 - binary_accuracy: 0.7319\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4756 - binary_accuracy: 0.7736\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4703 - binary_accuracy: 0.7473\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4521 - binary_accuracy: 0.7912\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4971 - binary_accuracy: 0.7538\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4416 - binary_accuracy: 0.7890\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4687 - binary_accuracy: 0.7846\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4278 - binary_accuracy: 0.8000\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4057 - binary_accuracy: 0.8176\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4300 - binary_accuracy: 0.7890\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3908 - binary_accuracy: 0.8352\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3840 - binary_accuracy: 0.8374\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4092 - binary_accuracy: 0.8088\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4060 - binary_accuracy: 0.8044\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4147 - binary_accuracy: 0.7890\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4168 - binary_accuracy: 0.8264\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3745 - binary_accuracy: 0.8396\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3872 - binary_accuracy: 0.8462\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3750 - binary_accuracy: 0.8352\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3652 - binary_accuracy: 0.8374\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3711 - binary_accuracy: 0.8571\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3790 - binary_accuracy: 0.8418\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3488 - binary_accuracy: 0.8703\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3449 - binary_accuracy: 0.8440\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3480 - binary_accuracy: 0.8462\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3384 - binary_accuracy: 0.8615\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3514 - binary_accuracy: 0.8571\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3426 - binary_accuracy: 0.8549\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3309 - binary_accuracy: 0.8703\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3495 - binary_accuracy: 0.8462\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3231 - binary_accuracy: 0.8659\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3313 - binary_accuracy: 0.8681\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3033 - binary_accuracy: 0.8703\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3029 - binary_accuracy: 0.8725\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3197 - binary_accuracy: 0.8615\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3117 - binary_accuracy: 0.8813\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3053 - binary_accuracy: 0.8769\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2938 - binary_accuracy: 0.8857\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3242 - binary_accuracy: 0.8527\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3097 - binary_accuracy: 0.8835\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2941 - binary_accuracy: 0.8769\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2879 - binary_accuracy: 0.8769\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2847 - binary_accuracy: 0.8835\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2975 - binary_accuracy: 0.8725\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2887 - binary_accuracy: 0.8769\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2724 - binary_accuracy: 0.8901\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3172 - binary_accuracy: 0.8681\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2844 - binary_accuracy: 0.8769\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2706 - binary_accuracy: 0.8923\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2729 - binary_accuracy: 0.8813\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2707 - binary_accuracy: 0.8967\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2839 - binary_accuracy: 0.8725\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2562 - binary_accuracy: 0.9099\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2384 - binary_accuracy: 0.9121\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2961 - binary_accuracy: 0.8857\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2579 - binary_accuracy: 0.9011\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2807 - binary_accuracy: 0.8945\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2659 - binary_accuracy: 0.8835\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2450 - binary_accuracy: 0.9165\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2585 - binary_accuracy: 0.8989\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2489 - binary_accuracy: 0.8967\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2522 - binary_accuracy: 0.8923\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2633 - binary_accuracy: 0.9055\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2497 - binary_accuracy: 0.9033\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2372 - binary_accuracy: 0.9055\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2400 - binary_accuracy: 0.9055\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2721 - binary_accuracy: 0.8769\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2479 - binary_accuracy: 0.9011\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2572 - binary_accuracy: 0.8857\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2427 - binary_accuracy: 0.9121\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2388 - binary_accuracy: 0.9121\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2623 - binary_accuracy: 0.8901\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2451 - binary_accuracy: 0.9077\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2380 - binary_accuracy: 0.9187\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2321 - binary_accuracy: 0.9209\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2282 - binary_accuracy: 0.9055\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2450 - binary_accuracy: 0.8967\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2678 - binary_accuracy: 0.9099\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2361 - binary_accuracy: 0.8989\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2301 - binary_accuracy: 0.8967\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2398 - binary_accuracy: 0.9099\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2302 - binary_accuracy: 0.9055\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2487 - binary_accuracy: 0.8989\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2590 - binary_accuracy: 0.8813\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2484 - binary_accuracy: 0.9033\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2567 - binary_accuracy: 0.9055\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2256 - binary_accuracy: 0.9297\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2259 - binary_accuracy: 0.9165\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adamax.py:95: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adamax, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46/46 [==============================] - 1s 2ms/step - loss: 2.2119 - binary_accuracy: 0.4846\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 1.0200 - binary_accuracy: 0.5439\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.9453 - binary_accuracy: 0.5768\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.6756 - binary_accuracy: 0.6294\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.6308 - binary_accuracy: 0.6469\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5821 - binary_accuracy: 0.6754\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5619 - binary_accuracy: 0.6645\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5676 - binary_accuracy: 0.6579\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5675 - binary_accuracy: 0.6272\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5331 - binary_accuracy: 0.6601\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5339 - binary_accuracy: 0.6820\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5854 - binary_accuracy: 0.6469\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5315 - binary_accuracy: 0.6689\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5320 - binary_accuracy: 0.6754\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5097 - binary_accuracy: 0.6842\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5260 - binary_accuracy: 0.6798\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5164 - binary_accuracy: 0.6864\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5072 - binary_accuracy: 0.7171\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5029 - binary_accuracy: 0.6996\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5119 - binary_accuracy: 0.6886\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5077 - binary_accuracy: 0.6996\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.5015 - binary_accuracy: 0.7083\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4822 - binary_accuracy: 0.6996\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4912 - binary_accuracy: 0.7434\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4901 - binary_accuracy: 0.7127\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4779 - binary_accuracy: 0.7412\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4877 - binary_accuracy: 0.7610\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4724 - binary_accuracy: 0.7325\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4453 - binary_accuracy: 0.7434\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4766 - binary_accuracy: 0.7259\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4573 - binary_accuracy: 0.7303\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4802 - binary_accuracy: 0.7412\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4664 - binary_accuracy: 0.7719\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4690 - binary_accuracy: 0.7873\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4460 - binary_accuracy: 0.7632\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4726 - binary_accuracy: 0.7610\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4663 - binary_accuracy: 0.7763\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4584 - binary_accuracy: 0.7632\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4568 - binary_accuracy: 0.7917\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4346 - binary_accuracy: 0.8048\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4466 - binary_accuracy: 0.7741\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4349 - binary_accuracy: 0.7939\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4534 - binary_accuracy: 0.7654\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4267 - binary_accuracy: 0.8004\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4297 - binary_accuracy: 0.8026\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4460 - binary_accuracy: 0.8048\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4212 - binary_accuracy: 0.8070\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4320 - binary_accuracy: 0.8004\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4206 - binary_accuracy: 0.8355\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4437 - binary_accuracy: 0.8158\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4318 - binary_accuracy: 0.8202\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4211 - binary_accuracy: 0.8070\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4114 - binary_accuracy: 0.8224\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4104 - binary_accuracy: 0.8114\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4035 - binary_accuracy: 0.8202\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3989 - binary_accuracy: 0.8158\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3955 - binary_accuracy: 0.8224\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.4169 - binary_accuracy: 0.8246\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3937 - binary_accuracy: 0.8575\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3725 - binary_accuracy: 0.8399\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3983 - binary_accuracy: 0.8268\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3963 - binary_accuracy: 0.8224\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3805 - binary_accuracy: 0.8596\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3939 - binary_accuracy: 0.8311\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3761 - binary_accuracy: 0.8575\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3685 - binary_accuracy: 0.8728\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3726 - binary_accuracy: 0.8487\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3772 - binary_accuracy: 0.8509\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3684 - binary_accuracy: 0.8706\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3755 - binary_accuracy: 0.8640\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3843 - binary_accuracy: 0.8399\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3612 - binary_accuracy: 0.8531\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3718 - binary_accuracy: 0.8904\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3549 - binary_accuracy: 0.8640\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3747 - binary_accuracy: 0.8421\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3541 - binary_accuracy: 0.8333\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3599 - binary_accuracy: 0.8596\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.3695 - binary_accuracy: 0.8640\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.3864 - binary_accuracy: 0.8289\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3571 - binary_accuracy: 0.8509\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3458 - binary_accuracy: 0.8640\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.3412 - binary_accuracy: 0.8728\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3432 - binary_accuracy: 0.8684\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3514 - binary_accuracy: 0.8772\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3321 - binary_accuracy: 0.8838\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.3137 - binary_accuracy: 0.8969\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3612 - binary_accuracy: 0.8596\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3460 - binary_accuracy: 0.8684\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3488 - binary_accuracy: 0.8750\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3414 - binary_accuracy: 0.8794\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3684 - binary_accuracy: 0.8509\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3220 - binary_accuracy: 0.8925\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3263 - binary_accuracy: 0.8860\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3269 - binary_accuracy: 0.8706\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3104 - binary_accuracy: 0.8969\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3189 - binary_accuracy: 0.8794\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3246 - binary_accuracy: 0.8816\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3149 - binary_accuracy: 0.8750\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3069 - binary_accuracy: 0.8728\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3337 - binary_accuracy: 0.8706\n",
            "4/4 [==============================] - 0s 5ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFjZuHxoTpxf",
        "outputId": "b433b5c1-2f62-44cd-a3f6-48caae24a79e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.84210526, 0.88596491, 0.92105263, 0.9122807 , 0.92920354])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = np.array(data_in_breast.iloc[147:148,:])\n",
        "new_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_11NKlejcUxK",
        "outputId": "3c1470ad-6c60-4c32-86fe-1050da3e8363"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.495e+01, 1.877e+01, 9.784e+01, 6.895e+02, 8.138e-02, 1.167e-01,\n",
              "        9.050e-02, 3.562e-02, 1.744e-01, 6.493e-02, 4.220e+02, 1.909e+03,\n",
              "        3.271e+03, 3.943e+01, 5.790e-03, 4.877e-02, 5.303e-02, 1.527e-02,\n",
              "        3.356e-02, 9.368e-03, 1.625e+01, 2.547e+01, 1.071e+02, 8.097e+02,\n",
              "        9.970e-02, 2.521e-01, 2.500e-01, 8.405e-02, 2.852e-01, 9.218e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_data = np.array(data_out_breast.iloc[147:148,:])\n",
        "result_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9abRpYQcYDO",
        "outputId": "b309ba1b-f2da-4193-c224-8ac8bbab82d7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.predict(new_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAWq9XZNdmW3",
        "outputId": "f4fc8f00-8fdf-4b21-ab92-d7278be82959"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 19ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.69046074]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data2 = np.array(data_in_breast.iloc[2:3,:])\n",
        "new_data2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f0e7154-d5d7-4f06-ef28-7d598809e41e",
        "id": "21I8O5aseuWP"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.969e+01, 2.125e+01, 1.300e+02, 1.203e+03, 1.096e-01, 1.599e-01,\n",
              "        1.974e-01, 1.279e-01, 2.069e-01, 5.999e-02, 7.456e-01, 7.869e-01,\n",
              "        4.585e+03, 9.403e+01, 6.150e-03, 4.006e-02, 3.832e-02, 2.058e-02,\n",
              "        2.250e-02, 4.571e-03, 2.357e+01, 2.553e+01, 1.525e+02, 1.709e+03,\n",
              "        1.444e-01, 4.245e-01, 4.504e-01, 2.430e+02, 3.613e-01, 8.758e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_data2 = np.array(data_out_breast.iloc[2:3,:])\n",
        "result_data2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ec00902-348f-4ea2-a6e1-fd88381f37f8",
        "id": "SoG01KcieuWQ"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.predict(new_data2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dab5d45d-d016-472f-fd75-8d2721229cae",
        "id": "cbq4IhlHeuWR"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.4249418e-06]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}